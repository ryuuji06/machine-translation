# Machine Translation with Bahdanau Attention Model

In this repository, I implement an automatic translation system from English to another language. It is currently trained to translate to portuguese, but it admits the translation to any language, provided available training data.

Literature: how it was traditionally done? How is it done now?

RNN, sequence-to-sequence models, then sequence-to-sequence with attention, and more recently it
is performed with transformers


Dataset we use. Pre-trained embedding: GloVe 6B, that learns the vector representation of words unsupervisedly.

Describe more the model.


## References

[x] Translation pairs dataset. http://www.manythings.org/anki/

[x] Pre-trained embeddings source. https://nlp.stanford.edu/projects/glove/

[] https://keras.io/examples/nlp/lstm_seq2seq/

[] https://www.tensorflow.org/text/tutorials/nmt_with_attention
